{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# useful for development to reload modules everytime we run code\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "If any errors in imports, then execute the following package installs and execute the imports again:\n",
    "\n",
    "pip install -r requirements.txt -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports\n",
    "from pprint import pprint\n",
    "import datetime, io, os, sys, time, json, uuid, glob\n",
    "from datetime import date\n",
    "\n",
    "# azure batch\n",
    "import azure.batch.batch_auth as batchauth\n",
    "import azure.batch._batch_service_client as batch\n",
    "import azure.batch.models as batchmodels\n",
    "\n",
    "# azure storage\n",
    "import azure.storage.blob as azureblob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Configuration\n",
    "Before executing, make sure settings.json file is correctly configured to access your resources on Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'batchAccountKey': 'Z5Gevll0QrJt0KzkXMi+EwbPbHXN9FDRPgvNOiDeMttT/ubg3tPr54ETbVwxpXub8tMHlZ8IGo026B29ThmKFQ==',\n",
      "  'batchAccountName': 'covidiabatch',\n",
      "  'batchServiceUrl': 'https://covidiabatch.francecentral.batch.azure.com',\n",
      "  'dedicatedVmCount': 0,\n",
      "  'jobIdPrefix': 'SimulationJob',\n",
      "  'lowPriorityVmCount': 5,\n",
      "  'poolId': 'SimulationPoolInfluenza',\n",
      "  'storageAccountKey': '6mZDnh2bYaunEgt+WTX7EqeBMw944ywtrZsaFyvbAeAWmr/Sg1g6ceRrOYkr3KUowjPvUBYZufNEC29B1jwdbg==',\n",
      "  'storageAccountName': 'covidiabatchstorage',\n",
      "  'vmSize': 'STANDARD_A1_V2'}\n"
     ]
    }
   ],
   "source": [
    "settings_file = \"settings.json\"\n",
    "\n",
    "with open(settings_file) as f:\n",
    "    settings = json.load(f)\n",
    "\n",
    "pprint(settings, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the batch client to create pools, jobs and tasks on Azure Batch\n",
    "credentials = batchauth.SharedKeyCredentials(\n",
    "       settings[\"batchAccountName\"],\n",
    "       settings[\"batchAccountKey\"])\n",
    "\n",
    "batch_client = batch.BatchServiceClient(\n",
    "        credentials,\n",
    "        batch_url=settings[\"batchServiceUrl\"])\n",
    "\n",
    "# Create the blob client, for use in obtaining references to\n",
    "# blob storage containers and uploading files to containers.\n",
    "\n",
    "blob_client = azureblob.BlockBlobService(\n",
    "    account_name = settings[\"storageAccountName\"],\n",
    "    account_key = settings[\"storageAccountKey\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Storage Containers\n",
    "This will create an application container to hold the application files that will be downloaded by the batch nodes.\n",
    "Additionally it will create an input and output container to store any input files to feed to each task and to collect the output of the tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Containers in Storage Account:\n",
      "\t application\n",
      "\t input\n",
      "\t output\n"
     ]
    }
   ],
   "source": [
    "# Use the blob client to create the containers in Azure Storage if they\n",
    "# don't yet exist.\n",
    "app_container_name = \"application\"\n",
    "input_container_name = \"input\"\n",
    "output_container_name = \"output\"\n",
    "\n",
    "blob_client.create_container(app_container_name, fail_on_exist=False)\n",
    "blob_client.create_container(input_container_name, fail_on_exist=False)\n",
    "blob_client.create_container(output_container_name, fail_on_exist=False)\n",
    "\n",
    "print(\"Containers in Storage Account:\")\n",
    "\n",
    "for c in blob_client.list_containers():\n",
    "    print(\"\\t\",c.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Azure Storage Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_blob_and_create_sas(block_blob_client, container_name, file_name, blob_name, hours=24):\n",
    "\n",
    "    block_blob_client.create_container(\n",
    "        container_name,\n",
    "        fail_on_exist=False)\n",
    "\n",
    "    block_blob_client.create_blob_from_path(\n",
    "        container_name,\n",
    "        blob_name,\n",
    "        file_name)\n",
    "\n",
    "    print(\"Uploaded\", file_name, \"to container\", container_name)\n",
    "\n",
    "    expiry = datetime.datetime.utcnow() + datetime.timedelta(hours=hours)\n",
    "    sas_token = block_blob_client.generate_blob_shared_access_signature(\n",
    "        container_name,\n",
    "        blob_name,\n",
    "        permission=azureblob.BlobPermissions.READ,\n",
    "        expiry=expiry)\n",
    "\n",
    "    sas_url = block_blob_client.make_blob_url(\n",
    "        container_name,\n",
    "        blob_name,\n",
    "        sas_token=sas_token)\n",
    "\n",
    "    return sas_url\n",
    "\n",
    "def create_container_sas_token(block_blob_client, container_name, permission, hours=24):\n",
    " \n",
    "    expiry = datetime.datetime.utcnow() + datetime.timedelta(hours=hours)\n",
    "    sas_token = block_blob_client.generate_container_shared_access_signature(\n",
    "        container_name, permission=permission, expiry=expiry)\n",
    "\n",
    "    valid_sas_url = \"https://{}.blob.core.windows.net/{}?{}\".format(\n",
    "        block_blob_client.account_name, container_name, sas_token\n",
    "    )\n",
    "    \n",
    "    return valid_sas_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copying app files to app container\n",
    "This step will zip the files in the app_dir folder and upload the zip package to the application container created in the previous step. We will also generate a script file to install Python on the Batch nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder to be zipped\n",
    "app_dir = \"sample_application\"\n",
    "# zip file name\n",
    "app_package_file_name = \"app.tar.gz\"\n",
    "# python install script\n",
    "setup_file_name = \"installPython.sh\"\n",
    "# folder to store the zipped file and install script\n",
    "resource_folder = \"batch_resources\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 20\n",
      "drwxrwxr-x 3 quick quick 4096 Apr 14 17:03 .\n",
      "drwxrwxr-x 6 quick quick 4096 Apr 14 21:17 ..\n",
      "-rw-rw-r-- 1 quick quick 1333 Apr 14 11:28 main.py\n",
      "-rw-rw-r-- 1 quick quick    6 Apr 14 17:03 requirements.txt\n",
      "drwxrwxr-x 3 quick quick 4096 Apr 14 11:10 sample_utils\n",
      "mkdir: cannot create directory ‘batch_resources’: File exists\n",
      "zipping application\n"
     ]
    }
   ],
   "source": [
    "# zip the application and copy zip file to resource folder\n",
    "!ls -la $app_dir\n",
    "!mkdir $resource_folder\n",
    "!echo zipping application\n",
    "!tar czf $resource_folder/app.tar.gz -C ./$app_dir/ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell creates the Python setup script for Ubuntu nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting batch_resources/installPython.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile $resource_folder/$setup_file_name\n",
    "sudo apt-get update\n",
    "sudo su\n",
    "apt-get install python3.6\n",
    "apt-get install -y python3-pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded batch_resources/app.tar.gz to container application\n",
      "https://covidiabatchstorage.blob.core.windows.net/application/app.tar.gz?se=2020-04-15T21%3A18%3A05Z&sp=r&sv=2018-03-28&sr=b&sig=IJwH/LqJiViNUHi9VY%2B74cf1Ybtfd2rfBiJDMzgaJt0%3D\n",
      "Uploaded batch_resources/installPython.sh to container application\n",
      "https://covidiabatchstorage.blob.core.windows.net/application/installPython.sh?se=2020-04-15T21%3A18%3A05Z&sp=r&sv=2018-03-28&sr=b&sig=GMCSWgAeTE6406XbIqHNdLka81PT1eymx/GpnZBDt0E%3D\n"
     ]
    }
   ],
   "source": [
    "# get file paths for upload\n",
    "app_file_path = os.path.join(resource_folder, app_package_file_name)\n",
    "setup_file_path = os.path.join(resource_folder, setup_file_name)\n",
    "\n",
    "# upload application package file to application container\n",
    "appFileSas = upload_blob_and_create_sas(blob_client, app_container_name, app_file_path, app_package_file_name)\n",
    "print(appFileSas)\n",
    "\n",
    "# upload install script to application container\n",
    "setupFileSas = upload_blob_and_create_sas(blob_client, app_container_name, setup_file_path, setup_file_name)\n",
    "print(setupFileSas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Azure Batch Pool\n",
    "A pool is the central compute resource for Azure Batch. It's composed of several machines that will be assigned tasks once a job is created.\n",
    "In here, we setup a pool of Ubuntu nodes and create a start task to make sure Python is installed. As machines get added to the pool, this task will imediately run before any tasks are assigned to the nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a StartTask\n",
    "Runs on all nodes on startup. This will reference the install script to make sure Python is installed on each node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start task:\n",
      "CommandLine: /bin/bash -c \"sudo sh installPython.sh\"\n",
      "ResourceFiles:\n",
      "\thttps://covidiabatchstorage.blob.core.windows.net/application/installPython.sh?se=2020-04-15T21%3A18%3A05Z&sp=r&sv=2018-03-28&sr=b&sig=GMCSWgAeTE6406XbIqHNdLka81PT1eymx/GpnZBDt0E%3D\n"
     ]
    }
   ],
   "source": [
    "# create an elevated identity to run the start task - needed whenever you require sudo access\n",
    "user = batchmodels.AutoUserSpecification(scope=batchmodels.AutoUserScope.pool, elevation_level=batchmodels.ElevationLevel.admin)\n",
    "user_identity = batchmodels.UserIdentity(auto_user=user)   \n",
    "\n",
    "# setup the task command - executing the shell script that install python\n",
    "command_line = f\"/bin/bash -c \\\"sudo sh {setup_file_name}\\\"\"\n",
    "\n",
    "# setup the start task\n",
    "startTask = batchmodels.StartTask(\n",
    "        command_line=command_line,\n",
    "        wait_for_success = True,\n",
    "        user_identity = user_identity,\n",
    "        resource_files = [batchmodels.ResourceFile(\n",
    "                         file_path = setup_file_name,\n",
    "                         http_url = setupFileSas)])\n",
    "\n",
    "print(\"Start task:\")\n",
    "print(f\"CommandLine: {command_line}\")\n",
    "print(f\"ResourceFiles:\")\n",
    "for f in startTask.resource_files:\n",
    "    print(f\"\\t{f.http_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating pool SimulationPoolInfluenza with:\n",
      "Size: STANDARD_A1_V2\n",
      "Number of dedicated nodes: 0\n",
      "Number of low priority nodes: 5\n"
     ]
    }
   ],
   "source": [
    "# checking configuration\n",
    "poolId = settings[\"poolId\"]\n",
    "vmSize = settings[\"vmSize\"]\n",
    "dedicatedNodes = settings[\"dedicatedVmCount\"]\n",
    "lowPriorityNodes = settings[\"lowPriorityVmCount\"]\n",
    "\n",
    "print(f\"Creating pool {poolId} with:\")\n",
    "print(\"Size:\",vmSize)\n",
    "print(\"Number of dedicated nodes:\",dedicatedNodes)\n",
    "print(\"Number of low priority nodes:\",lowPriorityNodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to create pool: SimulationPoolInfluenza\n",
      "Created pool: SimulationPoolInfluenza\n"
     ]
    }
   ],
   "source": [
    "# setup pool\n",
    "pool = batchmodels.PoolAddParameter(\n",
    "    id=poolId,\n",
    "    virtual_machine_configuration=batchmodels.VirtualMachineConfiguration(\n",
    "        image_reference=batchmodels.ImageReference(\n",
    "            publisher=\"Canonical\",\n",
    "            offer=\"UbuntuServer\",\n",
    "            sku=\"18.04-LTS\",\n",
    "            version=\"latest\"\n",
    "        ),\n",
    "        node_agent_sku_id=\"batch.node.ubuntu 18.04\"),\n",
    "    vm_size=vmSize,\n",
    "    target_dedicated_nodes=dedicatedNodes,\n",
    "    target_low_priority_nodes=lowPriorityNodes,\n",
    "    start_task=startTask)\n",
    "\n",
    "# create pool\n",
    "try:\n",
    "    print(\"Attempting to create pool:\", pool.id)\n",
    "    batch_client.pool.add(pool)\n",
    "    print(\"Created pool:\", pool.id)\n",
    "except batchmodels.BatchErrorException as e:\n",
    "    if e.error.code != \"PoolExists\":\n",
    "        raise\n",
    "    else:\n",
    "        print(\"Pool {!r} already exists\".format(pool.id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for all nodes in pool SimulationPoolInfluenza to reach one of: [<ComputeNodeState.idle: 'idle'>, <ComputeNodeState.running: 'running'>]\n",
      "\n",
      "waiting for 5 nodes to reach desired state...\n",
      "waiting for 5 nodes to reach desired state...\n",
      "waiting for 5 nodes to reach desired state...\n",
      "waiting for 5 nodes to reach desired state...\n",
      "waiting for 5 nodes to reach desired state...\n",
      "waiting for 5 nodes to reach desired state...\n",
      "waiting for 5 nodes to reach desired state...\n",
      "waiting for 5 nodes to reach desired state...\n",
      "waiting for 5 nodes to reach desired state...\n",
      "waiting for 5 nodes to reach desired state...\n",
      "tvmps_0849b14a971d7c65d2da7513d62c33501aa62f9e9cf3b3ed60f7f5e530ebbb63_p ComputeNodeState.idle False\n",
      "tvmps_88046169e06a8b41eb63ef7128fb1d4f3a1ed2fe559daf1ee956d8c24478a811_p ComputeNodeState.idle False\n",
      "tvmps_8a17a9d935ba6ca08b32d9535ef239e5e3908c6828ee29c921e81717622ca28d_p ComputeNodeState.idle False\n",
      "tvmps_ed14c4e4f566dd46ca4eb7940ea372521166ee5be80080f2522a1131759dd612_p ComputeNodeState.idle False\n",
      "tvmps_f04aff94082072bd4cb60fa687b061c020f234f9991ad542b8ffcda87a33e430_p ComputeNodeState.idle False\n"
     ]
    }
   ],
   "source": [
    "def wait_for_all_nodes_state(batch_client, pool, node_state):\n",
    "    print('Waiting for all nodes in pool {} to reach one of: {!r}\\n'.format(\n",
    "        pool.id, node_state))\n",
    "    i = 0\n",
    "    targetNodes = pool.target_dedicated_nodes + pool.target_low_priority_nodes\n",
    "    while True:\n",
    "        # refresh pool to ensure that there is no resize error\n",
    "        pool = batch_client.pool.get(pool.id)\n",
    "        if pool.resize_errors is not None:\n",
    "            resize_errors = \"\\n\".join([repr(e) for e in pool.resize_errors])\n",
    "            raise RuntimeError(\n",
    "                'resize error encountered for pool {}:\\n{}'.format(\n",
    "                    pool.id, resize_errors))\n",
    "        nodes = list(batch_client.compute_node.list(pool.id))\n",
    "        if (len(nodes) >= targetNodes and\n",
    "                all(node.state in node_state for node in nodes)):\n",
    "            return nodes\n",
    "        i += 1\n",
    "        if i % 3 == 0:\n",
    "            print('waiting for {} nodes to reach desired state...'.format(\n",
    "                targetNodes))\n",
    "        time.sleep(10)\n",
    "\n",
    "# we check if all nodes are up before we continue\n",
    "nodes = wait_for_all_nodes_state(batch_client, pool, [batchmodels.ComputeNodeState.idle, batchmodels.ComputeNodeState.running])\n",
    "\n",
    "# show all nodes\n",
    "for n in nodes:\n",
    "    print(n.id, n.state, n.is_dedicated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Job to run on the Pool\n",
    "We will now create a job and an associated Prep task to ensure the application is downloaded, extracted to a known location and all python packages are installed via pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Preparation task:\n",
      "CommandLine: /bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && tar -xvf $AZ_BATCH_JOB_PREP_WORKING_DIR/app.tar.gz && sudo su && pip3 install -r requirements.txt \"\n",
      "ResourceFiles:\n",
      "\thttps://covidiabatchstorage.blob.core.windows.net/application/app.tar.gz?se=2020-04-15T21%3A18%3A05Z&sp=r&sv=2018-03-28&sr=b&sig=IJwH/LqJiViNUHi9VY%2B74cf1Ybtfd2rfBiJDMzgaJt0%3D\n"
     ]
    }
   ],
   "source": [
    "# creating a unique job Id\n",
    "job_id = settings[\"jobIdPrefix\"] + \"_\" + str(date.today().year) + \"_\" + str(date.today().month) + \"_\" + str(date.today().day) + \"_\" + str(uuid.uuid1())\n",
    "\n",
    "# setup the task command\n",
    "command_line = f\"/bin/bash -c \\\"cd $AZ_BATCH_NODE_SHARED_DIR && tar -xvf $AZ_BATCH_JOB_PREP_WORKING_DIR/{app_package_file_name} && sudo su && pip3 install -r requirements.txt \\\"\"\n",
    "\n",
    "# create an elevated identity to run the start task\n",
    "user = batchmodels.AutoUserSpecification(scope=batchmodels.AutoUserScope.pool, elevation_level=batchmodels.ElevationLevel.admin)\n",
    "user_identity = batchmodels.UserIdentity(auto_user=user)   \n",
    "\n",
    "# setup the start task\n",
    "jobTask = batchmodels.JobPreparationTask(\n",
    "        command_line = command_line,\n",
    "        user_identity = user_identity,\n",
    "        wait_for_success = True,\n",
    "        resource_files = [batchmodels.ResourceFile(\n",
    "                         file_path = app_package_file_name,\n",
    "                         http_url = appFileSas)])\n",
    "\n",
    "print(\"Job Preparation task:\")\n",
    "print(f\"CommandLine: {command_line}\")\n",
    "print(f\"ResourceFiles:\")\n",
    "for f in jobTask.resource_files:\n",
    "    print(f\"\\t{f.http_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating job [SimulationJob_2020_4_14_2efaaf4e-7e96-11ea-a754-793f197936c4]...\n"
     ]
    }
   ],
   "source": [
    "# setup job\n",
    "job = batchmodels.JobAddParameter(\n",
    "    id=job_id,\n",
    "    pool_info=batchmodels.PoolInformation(pool_id=pool.id),\n",
    "    job_preparation_task = jobTask)\n",
    "\n",
    "# create job\n",
    "print('Creating job [{}]...'.format(job.id))\n",
    "\n",
    "try:\n",
    "    batch_client.job.add(job)\n",
    "except batchmodels.batch_error.BatchErrorException as err:\n",
    "    print_batch_exception(err)\n",
    "    if err.error.code != \"JobExists\":\n",
    "        raise\n",
    "    else:\n",
    "        print(\"Job {!r} already exists\".format(job_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Tasks to the Job\n",
    "Now that our application is correctly configured and we made sure Python is installed in all nodes, we need to setup a task to run a work item. We can launch many tasks inside the same job and Azure Batch will assign it to any VMs in the pool.\n",
    "\n",
    "In this example, we will create as many tasks as files in input_data (a local folder in this repo). This is a simple way of doing paralel processing of a large file when splits can be done. Another option is simple iterating over an array of parameter values and creating a task for each different value. We illustrate here the most complicated scenario which involves passing different input files to the script and uploading those files to the input container in the storage account.\n",
    "\n",
    "These tasks also write output to storage. The main.py script writes an output file and we configure the task to upload these files to the output container we created earlier. It is done after the task ends successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://covidiabatchstorage.blob.core.windows.net/output?se=2020-04-15T21%3A23%3A28Z&sp=w&sv=2018-03-28&sr=c&sig=st%2Be7/FQWQ%2ByrIWIj/iy9HnMHHzfARyYuO7aGKKyIpA%3D\n"
     ]
    }
   ],
   "source": [
    "# get a sas url for write access to output container. This will be used so we can persist task output files\n",
    "output_container_sas = create_container_sas_token(blob_client, container_name=output_container_name, permission=azureblob.BlobPermissions.WRITE)\n",
    "print(output_container_sas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating task Process-101\n",
      "Uploaded input_data/data56.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data56.dat -o $AZ_BATCH_TASK_WORKING_DIR/data56_output.csv\"\n",
      "\n",
      "Creating task Process-102\n",
      "Uploaded input_data/data43.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data43.dat -o $AZ_BATCH_TASK_WORKING_DIR/data43_output.csv\"\n",
      "\n",
      "Creating task Process-103\n",
      "Uploaded input_data/data8.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data8.dat -o $AZ_BATCH_TASK_WORKING_DIR/data8_output.csv\"\n",
      "\n",
      "Creating task Process-104\n",
      "Uploaded input_data/data35.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data35.dat -o $AZ_BATCH_TASK_WORKING_DIR/data35_output.csv\"\n",
      "\n",
      "Creating task Process-105\n",
      "Uploaded input_data/data33.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data33.dat -o $AZ_BATCH_TASK_WORKING_DIR/data33_output.csv\"\n",
      "\n",
      "Creating task Process-106\n",
      "Uploaded input_data/data23.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data23.dat -o $AZ_BATCH_TASK_WORKING_DIR/data23_output.csv\"\n",
      "\n",
      "Creating task Process-107\n",
      "Uploaded input_data/data25.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data25.dat -o $AZ_BATCH_TASK_WORKING_DIR/data25_output.csv\"\n",
      "\n",
      "Creating task Process-108\n",
      "Uploaded input_data/data34.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data34.dat -o $AZ_BATCH_TASK_WORKING_DIR/data34_output.csv\"\n",
      "\n",
      "Creating task Process-109\n",
      "Uploaded input_data/data57.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data57.dat -o $AZ_BATCH_TASK_WORKING_DIR/data57_output.csv\"\n",
      "\n",
      "Creating task Process-110\n",
      "Uploaded input_data/data6.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data6.dat -o $AZ_BATCH_TASK_WORKING_DIR/data6_output.csv\"\n",
      "\n",
      "Creating task Process-111\n",
      "Uploaded input_data/data59.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data59.dat -o $AZ_BATCH_TASK_WORKING_DIR/data59_output.csv\"\n",
      "\n",
      "Creating task Process-112\n",
      "Uploaded input_data/data3.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data3.dat -o $AZ_BATCH_TASK_WORKING_DIR/data3_output.csv\"\n",
      "\n",
      "Creating task Process-113\n",
      "Uploaded input_data/data30.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data30.dat -o $AZ_BATCH_TASK_WORKING_DIR/data30_output.csv\"\n",
      "\n",
      "Creating task Process-114\n",
      "Uploaded input_data/data47.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data47.dat -o $AZ_BATCH_TASK_WORKING_DIR/data47_output.csv\"\n",
      "\n",
      "Creating task Process-115\n",
      "Uploaded input_data/data10.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data10.dat -o $AZ_BATCH_TASK_WORKING_DIR/data10_output.csv\"\n",
      "\n",
      "Creating task Process-116\n",
      "Uploaded input_data/data26.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data26.dat -o $AZ_BATCH_TASK_WORKING_DIR/data26_output.csv\"\n",
      "\n",
      "Creating task Process-117\n",
      "Uploaded input_data/data48.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data48.dat -o $AZ_BATCH_TASK_WORKING_DIR/data48_output.csv\"\n",
      "\n",
      "Creating task Process-118\n",
      "Uploaded input_data/data31.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data31.dat -o $AZ_BATCH_TASK_WORKING_DIR/data31_output.csv\"\n",
      "\n",
      "Creating task Process-119\n",
      "Uploaded input_data/data28.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data28.dat -o $AZ_BATCH_TASK_WORKING_DIR/data28_output.csv\"\n",
      "\n",
      "Creating task Process-120\n",
      "Uploaded input_data/data2.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data2.dat -o $AZ_BATCH_TASK_WORKING_DIR/data2_output.csv\"\n",
      "\n",
      "Creating task Process-121\n",
      "Uploaded input_data/data58.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data58.dat -o $AZ_BATCH_TASK_WORKING_DIR/data58_output.csv\"\n",
      "\n",
      "Creating task Process-122\n",
      "Uploaded input_data/data13.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data13.dat -o $AZ_BATCH_TASK_WORKING_DIR/data13_output.csv\"\n",
      "\n",
      "Creating task Process-123\n",
      "Uploaded input_data/data46.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data46.dat -o $AZ_BATCH_TASK_WORKING_DIR/data46_output.csv\"\n",
      "\n",
      "Creating task Process-124\n",
      "Uploaded input_data/data29.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data29.dat -o $AZ_BATCH_TASK_WORKING_DIR/data29_output.csv\"\n",
      "\n",
      "Creating task Process-125\n",
      "Uploaded input_data/data41.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data41.dat -o $AZ_BATCH_TASK_WORKING_DIR/data41_output.csv\"\n",
      "\n",
      "Creating task Process-126\n",
      "Uploaded input_data/data44.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data44.dat -o $AZ_BATCH_TASK_WORKING_DIR/data44_output.csv\"\n",
      "\n",
      "Creating task Process-127\n",
      "Uploaded input_data/data39.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data39.dat -o $AZ_BATCH_TASK_WORKING_DIR/data39_output.csv\"\n",
      "\n",
      "Creating task Process-128\n",
      "Uploaded input_data/data49.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data49.dat -o $AZ_BATCH_TASK_WORKING_DIR/data49_output.csv\"\n",
      "\n",
      "Creating task Process-129\n",
      "Uploaded input_data/data20.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data20.dat -o $AZ_BATCH_TASK_WORKING_DIR/data20_output.csv\"\n",
      "\n",
      "Creating task Process-130\n",
      "Uploaded input_data/data19.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data19.dat -o $AZ_BATCH_TASK_WORKING_DIR/data19_output.csv\"\n",
      "\n",
      "Creating task Process-131\n",
      "Uploaded input_data/data38.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data38.dat -o $AZ_BATCH_TASK_WORKING_DIR/data38_output.csv\"\n",
      "\n",
      "Creating task Process-132\n",
      "Uploaded input_data/data15.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data15.dat -o $AZ_BATCH_TASK_WORKING_DIR/data15_output.csv\"\n",
      "\n",
      "Creating task Process-133\n",
      "Uploaded input_data/data37.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data37.dat -o $AZ_BATCH_TASK_WORKING_DIR/data37_output.csv\"\n",
      "\n",
      "Creating task Process-134\n",
      "Uploaded input_data/data24.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data24.dat -o $AZ_BATCH_TASK_WORKING_DIR/data24_output.csv\"\n",
      "\n",
      "Creating task Process-135\n",
      "Uploaded input_data/data12.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data12.dat -o $AZ_BATCH_TASK_WORKING_DIR/data12_output.csv\"\n",
      "\n",
      "Creating task Process-136\n",
      "Uploaded input_data/data7.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data7.dat -o $AZ_BATCH_TASK_WORKING_DIR/data7_output.csv\"\n",
      "\n",
      "Creating task Process-137\n",
      "Uploaded input_data/data53.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data53.dat -o $AZ_BATCH_TASK_WORKING_DIR/data53_output.csv\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating task Process-138\n",
      "Uploaded input_data/data11.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data11.dat -o $AZ_BATCH_TASK_WORKING_DIR/data11_output.csv\"\n",
      "\n",
      "Creating task Process-139\n",
      "Uploaded input_data/data40.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data40.dat -o $AZ_BATCH_TASK_WORKING_DIR/data40_output.csv\"\n",
      "\n",
      "Creating task Process-140\n",
      "Uploaded input_data/data18.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data18.dat -o $AZ_BATCH_TASK_WORKING_DIR/data18_output.csv\"\n",
      "\n",
      "Creating task Process-141\n",
      "Uploaded input_data/data52.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data52.dat -o $AZ_BATCH_TASK_WORKING_DIR/data52_output.csv\"\n",
      "\n",
      "Creating task Process-142\n",
      "Uploaded input_data/data9.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data9.dat -o $AZ_BATCH_TASK_WORKING_DIR/data9_output.csv\"\n",
      "\n",
      "Creating task Process-143\n",
      "Uploaded input_data/data54.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data54.dat -o $AZ_BATCH_TASK_WORKING_DIR/data54_output.csv\"\n",
      "\n",
      "Creating task Process-144\n",
      "Uploaded input_data/data16.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data16.dat -o $AZ_BATCH_TASK_WORKING_DIR/data16_output.csv\"\n",
      "\n",
      "Creating task Process-145\n",
      "Uploaded input_data/data45.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data45.dat -o $AZ_BATCH_TASK_WORKING_DIR/data45_output.csv\"\n",
      "\n",
      "Creating task Process-146\n",
      "Uploaded input_data/data55.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data55.dat -o $AZ_BATCH_TASK_WORKING_DIR/data55_output.csv\"\n",
      "\n",
      "Creating task Process-147\n",
      "Uploaded input_data/data21.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data21.dat -o $AZ_BATCH_TASK_WORKING_DIR/data21_output.csv\"\n",
      "\n",
      "Creating task Process-148\n",
      "Uploaded input_data/data36.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data36.dat -o $AZ_BATCH_TASK_WORKING_DIR/data36_output.csv\"\n",
      "\n",
      "Creating task Process-149\n",
      "Uploaded input_data/data51.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data51.dat -o $AZ_BATCH_TASK_WORKING_DIR/data51_output.csv\"\n",
      "\n",
      "Creating task Process-150\n",
      "Uploaded input_data/data32.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data32.dat -o $AZ_BATCH_TASK_WORKING_DIR/data32_output.csv\"\n",
      "\n",
      "Creating task Process-151\n",
      "Uploaded input_data/data4.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data4.dat -o $AZ_BATCH_TASK_WORKING_DIR/data4_output.csv\"\n",
      "\n",
      "Creating task Process-152\n",
      "Uploaded input_data/data50.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data50.dat -o $AZ_BATCH_TASK_WORKING_DIR/data50_output.csv\"\n",
      "\n",
      "Creating task Process-153\n",
      "Uploaded input_data/data1.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data1.dat -o $AZ_BATCH_TASK_WORKING_DIR/data1_output.csv\"\n",
      "\n",
      "Creating task Process-154\n",
      "Uploaded input_data/data14.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data14.dat -o $AZ_BATCH_TASK_WORKING_DIR/data14_output.csv\"\n",
      "\n",
      "Creating task Process-155\n",
      "Uploaded input_data/data22.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data22.dat -o $AZ_BATCH_TASK_WORKING_DIR/data22_output.csv\"\n",
      "\n",
      "Creating task Process-156\n",
      "Uploaded input_data/data5.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data5.dat -o $AZ_BATCH_TASK_WORKING_DIR/data5_output.csv\"\n",
      "\n",
      "Creating task Process-157\n",
      "Uploaded input_data/data27.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data27.dat -o $AZ_BATCH_TASK_WORKING_DIR/data27_output.csv\"\n",
      "\n",
      "Creating task Process-158\n",
      "Uploaded input_data/data0.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data0.dat -o $AZ_BATCH_TASK_WORKING_DIR/data0_output.csv\"\n",
      "\n",
      "Creating task Process-159\n",
      "Uploaded input_data/data42.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data42.dat -o $AZ_BATCH_TASK_WORKING_DIR/data42_output.csv\"\n",
      "\n",
      "Creating task Process-160\n",
      "Uploaded input_data/data17.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data17.dat -o $AZ_BATCH_TASK_WORKING_DIR/data17_output.csv\"\n"
     ]
    }
   ],
   "source": [
    "# we get a list of input files\n",
    "file_list = glob.glob(\"input_data/*.dat\")\n",
    "\n",
    "# initialize task counter\n",
    "i = 100\n",
    "for f in file_list:\n",
    "    # increment task counter\n",
    "    i = i + 1\n",
    "    \n",
    "    # create a task id\n",
    "    task_id = \"Process-\" + str(i)\n",
    "    print(\"\\nCreating task\",task_id)\n",
    "    \n",
    "    # grab file name\n",
    "    input_file = f.split(\"/\")[-1:][0]\n",
    "    output_file = input_file.replace(\".dat\",\"_output.csv\")\n",
    "    \n",
    "    # upload file to azure storage\n",
    "    input_file_sas = upload_blob_and_create_sas(blob_client, input_container_name, f, input_file)\n",
    "    \n",
    "    # setup task command\n",
    "    taskCommand = f\"/bin/bash -c \\\"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/{input_file} -o $AZ_BATCH_TASK_WORKING_DIR/{output_file}\\\"\"\n",
    "    print(taskCommand)\n",
    "    \n",
    "    # create an elevated identity to run the start task\n",
    "    user = batchmodels.AutoUserSpecification(scope=batchmodels.AutoUserScope.pool, elevation_level=batchmodels.ElevationLevel.admin)\n",
    "    user_identity = batchmodels.UserIdentity(auto_user=user)   \n",
    "   \n",
    "    # setup output files destination\n",
    "    containerDest = batchmodels.OutputFileBlobContainerDestination(container_url = output_container_sas, path = task_id)\n",
    "    outputFileDestination = batchmodels.OutputFileDestination(container = containerDest)\n",
    "    \n",
    "    # setup output files upload condition\n",
    "    uploadCondition = batchmodels.OutputFileUploadCondition.task_success\n",
    "    uploadOptions = batchmodels.OutputFileUploadOptions(upload_condition = uploadCondition)\n",
    "    \n",
    "    # output files\n",
    "    output_files = [batchmodels.OutputFile(destination = outputFileDestination,\n",
    "                                        upload_options = uploadOptions,\n",
    "                                        file_pattern=\"*output.csv\")]\n",
    "    \n",
    "    \n",
    "    # create task\n",
    "    task = batchmodels.TaskAddParameter(\n",
    "    id = task_id,\n",
    "    command_line=taskCommand,\n",
    "    user_identity=user_identity,\n",
    "    resource_files=[batchmodels.ResourceFile(\n",
    "                        file_path=input_file,\n",
    "                        http_url=input_file_sas)],\n",
    "    output_files=output_files)\n",
    "    \n",
    "    \n",
    "    batch_client.task.add(job_id=job.id, task=task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if all tasks are complete...\n",
      "Checking if all tasks are complete...\n",
      "Checking if all tasks are complete...\n",
      "Checking if all tasks are complete...\n",
      "Checking if all tasks are complete...\n",
      "Checking if all tasks are complete...\n",
      "All Tasks Complete!\n"
     ]
    }
   ],
   "source": [
    "def wait_for_tasks_to_complete(batch_client, job_id, timeout):\n",
    "    \"\"\"Waits for all the tasks in a particular job to complete.\n",
    "\n",
    "    :param batch_client: The batch client to use.\n",
    "    :type batch_client: `batchserviceclient.BatchServiceClient`\n",
    "    :param str job_id: The id of the job to monitor.\n",
    "    :param timeout: The maximum amount of time to wait.\n",
    "    :type timeout: `datetime.timedelta`\n",
    "    \"\"\"\n",
    "    time_to_timeout_at = datetime.datetime.now() + timeout\n",
    "\n",
    "    while datetime.datetime.now() < time_to_timeout_at:\n",
    "        print(\"Checking if all tasks are complete...\")\n",
    "        tasks = batch_client.task.list(job_id)\n",
    "\n",
    "        incomplete_tasks = [task for task in tasks if\n",
    "                            task.state != batchmodels.TaskState.completed]\n",
    "        if not incomplete_tasks:\n",
    "            return\n",
    "        time.sleep(30)\n",
    "\n",
    "    raise TimeoutError(\"Timed out waiting for tasks to complete\")\n",
    "\n",
    "wait_for_tasks_to_complete(batch_client, job.id, datetime.timedelta(minutes=60))\n",
    "print(\"All Tasks Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions to read task output directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stdout.txt content for task Process-101: \n",
      "Application version: 1.2\n",
      "Started at: 2020-04-14 21:23:58.448409\n",
      "\n",
      "Received data:\n",
      "\n",
      "      0   1         2   3   4   ...         13         14  15     16    17\n",
      "0   5700   2  Saturday  17  54  ... -73.782097  40.745861   1   6.08  19.0\n",
      "1   5701   2  Saturday  17  56  ... -73.847115  40.865940   1   2.76  12.0\n",
      "2   5702   2  Saturday  17  56  ... -73.865463  40.725395   1   1.67   8.0\n",
      "3   5703   2  Saturday  17  57  ... -73.999985  40.682884   1  13.03  38.0\n",
      "4   5704   2  Saturday  17  58  ... -73.809593  40.721497   1   2.35   8.5\n",
      "..   ...  ..       ...  ..  ..  ...        ...        ...  ..    ...   ...\n",
      "95  5795   2  Saturday  19  32  ... -73.950279  40.786373   5   4.25  13.5\n",
      "96  5796   1  Saturday  19  32  ... -73.862411  40.730431   1   2.00  11.0\n",
      "97  5797   2  Saturday  19  33  ... -73.884659  40.737160   5   3.18  13.5\n",
      "98  5798   2  Saturday  19  34  ... -73.935631  40.845116   1   5.79  16.0\n",
      "99  5799   2  Saturday  19  34  ... -73.922737  40.755783   1   1.01   5.5\n",
      "\n",
      "[100 rows x 18 columns]\n",
      "\n",
      "Doing something really important...\n",
      "\n",
      "\n",
      "Processed data:\n",
      "\n",
      "   original  squared\n",
      "0        54     2916\n",
      "1        56     3136\n",
      "2        56     3136\n",
      "3        57     3249\n",
      "4        58     3364\n",
      "5         0        0\n",
      "6         0        0\n",
      "7         0        0\n",
      "8         2        4\n",
      "9         3        9\n",
      "\n",
      "Wrote result to /mnt/batch/tasks/workitems/SimulationJob_2020_4_14_2efaaf4e-7e96-11ea-a754-793f197936c4/job-1/Process-101/wd/data56_output.csv\n",
      "\n",
      "Exited at: 2020-04-14 21:24:03.574181\n",
      "\n",
      "stderr.txt content for task Process-101: \n",
      "\n",
      "stdout.txt content for task Process-102: \n",
      "Application version: 1.2\n",
      "Started at: 2020-04-14 21:24:08.236751\n",
      "\n",
      "Received data:\n",
      "\n",
      "      0   1         2   3   4   ...         13         14  15     16    17\n",
      "0   4400   2  Thursday  20  43  ... -73.998238  40.693748   1   1.38   7.5\n",
      "1   4401   2  Thursday  20  43  ... -73.920227  40.757404   5   2.77  15.0\n",
      "2   4402   2  Thursday  20  46  ... -74.016670  40.676048   1   2.22   9.5\n",
      "3   4403   2  Thursday  20  48  ... -73.851219  40.699726   1   2.30   9.0\n",
      "4   4404   2  Thursday  20  48  ... -73.942368  40.819729   5   2.30  10.0\n",
      "..   ...  ..       ...  ..  ..  ...        ...        ...  ..    ...   ...\n",
      "95  4495   2  Thursday  23   6  ... -73.920303  40.794891   1   1.97   8.5\n",
      "96  4496   2  Thursday  23   9  ... -73.949623  40.744019   5   1.16   6.0\n",
      "97  4497   2  Thursday  23  10  ... -74.006134  40.719627   1  16.39  45.0\n",
      "98  4498   2  Thursday  23  11  ... -73.797394  40.715691   2   2.46  10.0\n",
      "99  4499   2  Thursday  23  14  ... -73.870392  40.718246   1   1.67   8.0\n",
      "\n",
      "[100 rows x 18 columns]\n",
      "\n",
      "Doing something really important...\n",
      "\n",
      "\n",
      "Processed data:\n",
      "\n",
      "   original  squared\n",
      "0        43     1849\n",
      "1        43     1849\n",
      "2        46     2116\n",
      "3        48     2304\n",
      "4        48     2304\n",
      "5        50     2500\n",
      "6        53     2809\n",
      "7        59     3481\n",
      "8         0        0\n",
      "9         2        4\n",
      "\n",
      "Wrote result to /mnt/batch/tasks/workitems/SimulationJob_2020_4_14_2efaaf4e-7e96-11ea-a754-793f197936c4/job-1/Process-102/wd/data43_output.csv\n",
      "\n",
      "Exited at: 2020-04-14 21:24:13.326762\n",
      "\n",
      "stderr.txt content for task Process-102: \n",
      "\n",
      "stdout.txt content for task Process-103: \n",
      "Application version: 1.2\n",
      "Started at: 2020-04-14 21:23:58.286462\n",
      "\n",
      "Received data:\n",
      "\n",
      "     0   1       2   3   4   ...         13         14  15     16    17\n",
      "0   900   2  Sunday  11  49  ... -73.876587  40.747883   1   1.89   8.5\n",
      "1   901   2  Sunday  12   1  ... -73.943672  40.801556   1   0.02   2.5\n",
      "2   902   2  Sunday  12  15  ... -73.933105  40.795223   1   1.53   7.0\n",
      "3   903   2  Sunday  12  27  ... -73.929832  40.854763   5   0.06   2.5\n",
      "4   904   2  Sunday  12  32  ... -73.979607  40.776344   1   3.72  14.5\n",
      "..  ...  ..     ...  ..  ..  ...        ...        ...  ..    ...   ...\n",
      "95  995   2  Monday   9  13  ... -73.938225  40.757076   4   0.10  52.0\n",
      "96  996   2  Monday   9  20  ... -73.871109  40.774132   1   5.60  19.5\n",
      "97  997   2  Monday   9  23  ... -73.805450  40.709980   1   2.31  10.0\n",
      "98  998   2  Monday   9  29  ... -73.958488  40.781471   5   2.68  13.0\n",
      "99  999   2  Monday   9  29  ... -73.987007  40.693512   5  10.67  33.5\n",
      "\n",
      "[100 rows x 18 columns]\n",
      "\n",
      "Doing something really important...\n",
      "\n",
      "\n",
      "Processed data:\n",
      "\n",
      "   original  squared\n",
      "0        49     2401\n",
      "1         1        1\n",
      "2        15      225\n",
      "3        27      729\n",
      "4        32     1024\n",
      "5        41     1681\n",
      "6        44     1936\n",
      "7        52     2704\n",
      "8        52     2704\n",
      "9        11      121\n",
      "\n",
      "Wrote result to /mnt/batch/tasks/workitems/SimulationJob_2020_4_14_2efaaf4e-7e96-11ea-a754-793f197936c4/job-1/Process-103/wd/data8_output.csv\n",
      "\n",
      "Exited at: 2020-04-14 21:24:03.376415\n",
      "\n",
      "stderr.txt content for task Process-103: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def read_stream_as_string(stream, encoding):\n",
    "    output = io.BytesIO()\n",
    "    try:\n",
    "        for data in stream:\n",
    "            output.write(data)\n",
    "        if encoding is None:\n",
    "            encoding = 'utf-8'\n",
    "        return output.getvalue().decode(encoding)\n",
    "    finally:\n",
    "        output.close()\n",
    "    raise RuntimeError('could not write data to stream or decode bytes')\n",
    "\n",
    "def read_task_file_as_string(batch_client, job_id, task_id, file_name, encoding=None):\n",
    "    stream = batch_client.file.get_from_task(job_id, task_id, file_name)\n",
    "    return _read_stream_as_string(stream, encoding)\n",
    "\n",
    "def print_task_output(batch_client, job_id, task_ids, encoding=None):\n",
    "    _STANDARD_OUT_FILE_NAME = 'stdout.txt'\n",
    "    _STANDARD_ERROR_FILE_NAME = 'stderr.txt'\n",
    "    \n",
    "    for task_id in task_ids:\n",
    "        file_text = read_task_file_as_string(\n",
    "            batch_client,\n",
    "            job_id,\n",
    "            task_id,\n",
    "            _STANDARD_OUT_FILE_NAME,\n",
    "            encoding)\n",
    "        print(\"{} content for task {}: \".format(\n",
    "            _STANDARD_OUT_FILE_NAME,\n",
    "            task_id))\n",
    "        print(file_text)\n",
    "\n",
    "        file_text = read_task_file_as_string(\n",
    "            batch_client,\n",
    "            job_id,\n",
    "            task_id,\n",
    "            _STANDARD_ERROR_FILE_NAME,\n",
    "            encoding)\n",
    "        print(\"{} content for task {}: \".format(\n",
    "            _STANDARD_ERROR_FILE_NAME,\n",
    "            task_id))\n",
    "        print(file_text)\n",
    "\n",
    "tasks = batch_client.task.list(job_id)\n",
    "\n",
    "# let's print the output of the first 3 tasks\n",
    "print_task_output(batch_client, job_id, task_ids[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Output\n",
    "As we created each task with an output file option, the produced filed by each execution of our sample_application will result in a new file being created in the output container in Azure Storage. We can quickly check all the files here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 60\n",
      "\n",
      "First 10:\n",
      "Process-101/data56_output.csv\n",
      "Process-102/data43_output.csv\n",
      "Process-103/data8_output.csv\n",
      "Process-104/data35_output.csv\n",
      "Process-105/data33_output.csv\n",
      "Process-106/data23_output.csv\n",
      "Process-107/data25_output.csv\n",
      "Process-108/data34_output.csv\n",
      "Process-109/data57_output.csv\n",
      "Process-110/data6_output.csv\n"
     ]
    }
   ],
   "source": [
    "output_file_list = blob_client.list_blobs(container_name=output_container_name)\n",
    "print(\"Number of files:\",len(list(output_file_list)))\n",
    "print(\"\\nFirst 10:\")\n",
    "for f in list(output_file_list)[0:10]:\n",
    "    print(f.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Job\n",
    "No issues in removing the job because each task will write it's results to the output container in Azure Storage, however, keeping this uncommented will allow you to see the job in Batch Explorer and debug any failed tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_client.job.delete(job.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Pool\n",
    "Note: you may not necessarily want to do this because creating the pool takes some time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_client.pool.delete(pool.id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureml_py36_automl",
   "language": "python",
   "name": "conda-env-azureml_py36_automl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
