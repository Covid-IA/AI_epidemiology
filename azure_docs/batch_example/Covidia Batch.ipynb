{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful for development to reload modules everytime we run code\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "If any errors in imports, then execute the following package installs and execute the imports again:\n",
    "\n",
    "pip install -r requirements.txt -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports\n",
    "from pprint import pprint\n",
    "import io, os, sys, time, json, uuid, glob\n",
    "from datetime import datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# azure batch\n",
    "import azure.batch.batch_auth as batchauth\n",
    "import azure.batch._batch_service_client as batch\n",
    "import azure.batch.models as batchmodels\n",
    "\n",
    "# azure storage\n",
    "import azure.storage.blob as azureblob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Configuration\n",
    "Before executing, make sure you create a .env file with the following format:\n",
    "\n",
    "```\n",
    "BATCH_SERVICE_URL=<batch_service_url>\n",
    "BATCH_ACCOUNT_NAME=<batch_account_name>\n",
    "BATCH_ACCOUNT_KEY=<batch_account_key>\n",
    "BATCH_STORAGE_ACCOUNT_NAME=<storage_account_name>\n",
    "BATCH_STORAGE_ACCOUNT_KEY=<storage_account_key>\n",
    "```\n",
    "\n",
    "This .env file is not added on the repo to avoid surfacing sensitive data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://covidiabatch.francecentral.batch.azure.com\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "batch_service_url = os.getenv(\"BATCH_SERVICE_URL\")\n",
    "batch_account_name = os.getenv(\"BATCH_ACCOUNT_NAME\")\n",
    "batch_account_key = os.getenv(\"BATCH_ACCOUNT_KEY\")\n",
    "storage_account_name = os.getenv(\"BATCH_STORAGE_ACCOUNT_NAME\")\n",
    "storage_account_key = os.getenv(\"BATCH_STORAGE_ACCOUNT_KEY\")\n",
    "\n",
    "print(batch_service_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the batch client to create pools, jobs and tasks on Azure Batch\n",
    "credentials = batchauth.SharedKeyCredentials(\n",
    "       batch_account_name,\n",
    "       batch_account_key)\n",
    "\n",
    "batch_client = batch.BatchServiceClient(\n",
    "        credentials,\n",
    "        batch_url=batch_service_url)\n",
    "\n",
    "# Create the blob client, for use in obtaining references to\n",
    "# blob storage containers and uploading files to containers.\n",
    "blob_client = azureblob.BlockBlobService(\n",
    "    account_name = storage_account_name,\n",
    "    account_key = storage_account_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Storage Containers\n",
    "This will create an application container to hold the application files that will be downloaded by the batch nodes.\n",
    "Additionally it will create an input and output container to store any input files to feed to each task and to collect the output of the tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Containers in Storage Account:\n",
      "\t application\n",
      "\t input\n",
      "\t output\n"
     ]
    }
   ],
   "source": [
    "# Use the blob client to create the containers in Azure Storage if they\n",
    "# don't yet exist.\n",
    "app_container_name = \"application\"\n",
    "input_container_name = \"input\"\n",
    "output_container_name = \"output\"\n",
    "\n",
    "blob_client.create_container(app_container_name, fail_on_exist=False)\n",
    "blob_client.create_container(input_container_name, fail_on_exist=False)\n",
    "blob_client.create_container(output_container_name, fail_on_exist=False)\n",
    "\n",
    "print(\"Containers in Storage Account:\")\n",
    "\n",
    "for c in blob_client.list_containers():\n",
    "    print(\"\\t\",c.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Azure Storage Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_blob_and_create_sas(block_blob_client, container_name, file_name, blob_name, hours=24):\n",
    "\n",
    "    block_blob_client.create_container(\n",
    "        container_name,\n",
    "        fail_on_exist=False)\n",
    "\n",
    "    block_blob_client.create_blob_from_path(\n",
    "        container_name,\n",
    "        blob_name,\n",
    "        file_name)\n",
    "\n",
    "    print(\"Uploaded\", file_name, \"to container\", container_name)\n",
    "\n",
    "    expiry = datetime.utcnow() + timedelta(hours=hours)\n",
    "    sas_token = block_blob_client.generate_blob_shared_access_signature(\n",
    "        container_name,\n",
    "        blob_name,\n",
    "        permission=azureblob.BlobPermissions.READ,\n",
    "        expiry=expiry)\n",
    "\n",
    "    sas_url = block_blob_client.make_blob_url(\n",
    "        container_name,\n",
    "        blob_name,\n",
    "        sas_token=sas_token)\n",
    "\n",
    "    return sas_url\n",
    "\n",
    "def create_container_sas_token(block_blob_client, container_name, permission, hours=24):\n",
    " \n",
    "    expiry = datetime.utcnow() + timedelta(hours=hours)\n",
    "    sas_token = block_blob_client.generate_container_shared_access_signature(\n",
    "        container_name, permission=permission, expiry=expiry)\n",
    "\n",
    "    valid_sas_url = \"https://{}.blob.core.windows.net/{}?{}\".format(\n",
    "        block_blob_client.account_name, container_name, sas_token\n",
    "    )\n",
    "    \n",
    "    return valid_sas_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copying app files to app container\n",
    "This step will zip the files in the app_dir folder and upload the zip package to the application container created in the previous step. We will also generate a script file to install Python on the Batch nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder to be zipped\n",
    "app_dir = \"sample_application\"\n",
    "# zip file name\n",
    "app_package_file_name = \"app.tar.gz\"\n",
    "# python install script\n",
    "setup_file_name = \"installPython.sh\"\n",
    "# folder to store the zipped file and install script\n",
    "resource_folder = \"batch_resources\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 20\n",
      "drwxrwxr-x 3 quick quick 4096 Apr 14 21:34 .\n",
      "drwxrwxr-x 6 quick quick 4096 Apr 14 22:57 ..\n",
      "-rw-rw-r-- 1 quick quick 1333 Apr 14 21:37 main.py\n",
      "-rw-rw-r-- 1 quick quick    6 Apr 14 17:03 requirements.txt\n",
      "drwxrwxr-x 3 quick quick 4096 Apr 14 11:10 sample_utils\n",
      "mkdir: cannot create directory ‘batch_resources’: File exists\n",
      "zipping application\n"
     ]
    }
   ],
   "source": [
    "# zip the application and copy zip file to resource folder\n",
    "!ls -la $app_dir\n",
    "!mkdir $resource_folder\n",
    "!echo zipping application\n",
    "!tar czf $resource_folder/app.tar.gz -C ./$app_dir/ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell creates the Python setup script for Ubuntu nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting batch_resources/installPython.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile $resource_folder/$setup_file_name\n",
    "sudo apt-get update\n",
    "sudo su\n",
    "apt-get install python3.6\n",
    "apt-get install -y python3-pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded batch_resources/app.tar.gz to container application\n",
      "https://covidiabatchstorage.blob.core.windows.net/application/app.tar.gz?se=2020-04-15T22%3A58%3A37Z&sp=r&sv=2018-03-28&sr=b&sig=9RRpXh1rPnACsnU9YuEvuglUI%2BOBT4z/HhcO/EL5EYg%3D\n",
      "Uploaded batch_resources/installPython.sh to container application\n",
      "https://covidiabatchstorage.blob.core.windows.net/application/installPython.sh?se=2020-04-15T22%3A58%3A37Z&sp=r&sv=2018-03-28&sr=b&sig=ZjIXmfzReYJWNeLIaoKBL70PmywvrlEb7VXHAkjG9TY%3D\n"
     ]
    }
   ],
   "source": [
    "# get file paths for upload\n",
    "app_file_path = os.path.join(resource_folder, app_package_file_name)\n",
    "setup_file_path = os.path.join(resource_folder, setup_file_name)\n",
    "\n",
    "# upload application package file to application container\n",
    "appFileSas = upload_blob_and_create_sas(blob_client, app_container_name, app_file_path, app_package_file_name)\n",
    "print(appFileSas)\n",
    "\n",
    "# upload install script to application container\n",
    "setupFileSas = upload_blob_and_create_sas(blob_client, app_container_name, setup_file_path, setup_file_name)\n",
    "print(setupFileSas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Azure Batch Pool\n",
    "A pool is the central compute resource for Azure Batch. It's composed of several machines that will be assigned tasks once a job is created.\n",
    "In here, we setup a pool of Ubuntu nodes and create a start task to make sure Python is installed. As machines get added to the pool, this task will imediately run before any tasks are assigned to the nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'dedicatedVmCount': 0,\n",
      "  'jobIdPrefix': 'SimulationJobInfluenza',\n",
      "  'lowPriorityVmCount': 5,\n",
      "  'poolId': 'SimulationPoolInfluenza',\n",
      "  'vmSize': 'STANDARD_A1_V2'}\n"
     ]
    }
   ],
   "source": [
    "# let's read the configuration\n",
    "settings_file = \"batch_settings.json\"\n",
    "\n",
    "with open(settings_file) as f:\n",
    "    settings = json.load(f)\n",
    "\n",
    "pprint(settings, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a StartTask\n",
    "Runs on all nodes on startup. This will reference the install script to make sure Python is installed on each node\n",
    "\n",
    "Note:\n",
    "*To enable detailed monitoring of pool nodes, we have to setup Application Insights as detailed here: https://github.com/Azure/batch-insights. This is not required but it's a good way to understand if we can run more tasks in paralel inside the same node. This requires setting up 3 environment variables on the pool start task. In this notebook we are setting the variables but we are not enabling app insights by default because it causes a significant spike in node start time (a few minutes compared to a few seconds). This won't be relevant in a real scenario where a job takes significant more time to run than the startup time of the pool, but in this demo it has a noticeable impact.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up app insights related environment variables in task\n",
    "env_variables = list()\n",
    "env_variables.append(batchmodels.EnvironmentSetting(name=\"APP_INSIGHTS_INSTRUMENTATION_KEY\", value=\"d5c240e1-1a2e-4e69-b5fd-9009627f03ec\"))\n",
    "env_variables.append(batchmodels.EnvironmentSetting(name=\"APP_INSIGHTS_APP_ID\", value=\"c32ac08b-472e-4128-b245-62c3ed03a8a5\"))\n",
    "env_variables.append(batchmodels.EnvironmentSetting(name=\"BATCH_INSIGHTS_DOWNLOAD_URL\", value=\"https://github.com/Azure/batch-insights/releases/download/v1.3.0/batch-insights\"))                                     \n",
    "\n",
    "# notice we could use this to setup any environment variables that the task would need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start task:\n",
      "CommandLine: /bin/bash -c \"sudo sh installPython.sh && wget -O - https://raw.githubusercontent.com/Azure/batch-insights/master/scripts/run-linux.sh | bash \"\n",
      "ResourceFiles:\n",
      "\thttps://covidiabatchstorage.blob.core.windows.net/application/installPython.sh?se=2020-04-15T22%3A58%3A37Z&sp=r&sv=2018-03-28&sr=b&sig=ZjIXmfzReYJWNeLIaoKBL70PmywvrlEb7VXHAkjG9TY%3D\n"
     ]
    }
   ],
   "source": [
    "# create an elevated identity to run the start task - needed whenever you require sudo access\n",
    "user = batchmodels.AutoUserSpecification(scope=batchmodels.AutoUserScope.pool, elevation_level=batchmodels.ElevationLevel.admin)\n",
    "user_identity = batchmodels.UserIdentity(auto_user=user)   \n",
    "\n",
    "# setup the task command - executing the shell script that install python. \n",
    "command_line = f\"/bin/bash -c \\\"sudo sh {setup_file_name}\\\"\"\n",
    "\n",
    "# alternative command line to setup application insights - uncomment this to enable app insights\n",
    "#command_line = f\"/bin/bash -c \\\"sudo sh {setup_file_name} && wget -O - https://raw.githubusercontent.com/Azure/batch-insights/master/scripts/run-linux.sh | bash \\\"\"\n",
    "\n",
    "# setup the start task\n",
    "startTask = batchmodels.StartTask(\n",
    "        command_line=command_line,\n",
    "        wait_for_success = True,\n",
    "        user_identity = user_identity,\n",
    "        environment_settings=env_variables,\n",
    "        resource_files = [batchmodels.ResourceFile(\n",
    "                         file_path = setup_file_name,\n",
    "                         http_url = setupFileSas)])\n",
    "\n",
    "print(\"Start task:\")\n",
    "print(f\"CommandLine: {command_line}\")\n",
    "print(f\"ResourceFiles:\")\n",
    "for f in startTask.resource_files:\n",
    "    print(f\"\\t{f.http_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating pool SimulationPoolInfluenza with:\n",
      "Size: STANDARD_A1_V2\n",
      "Number of dedicated nodes: 0\n",
      "Number of low priority nodes: 5\n"
     ]
    }
   ],
   "source": [
    "# checking configuration\n",
    "poolId = settings[\"poolId\"]\n",
    "vmSize = settings[\"vmSize\"]\n",
    "dedicatedNodes = settings[\"dedicatedVmCount\"]\n",
    "lowPriorityNodes = settings[\"lowPriorityVmCount\"]\n",
    "\n",
    "print(f\"Creating pool {poolId} with:\")\n",
    "print(\"Size:\",vmSize)\n",
    "print(\"Number of dedicated nodes:\",dedicatedNodes)\n",
    "print(\"Number of low priority nodes:\",lowPriorityNodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to create pool: SimulationPoolInfluenza\n",
      "Created pool: SimulationPoolInfluenza\n"
     ]
    }
   ],
   "source": [
    "# setup pool\n",
    "pool = batchmodels.PoolAddParameter(\n",
    "    id=poolId,\n",
    "    virtual_machine_configuration=batchmodels.VirtualMachineConfiguration(\n",
    "        image_reference=batchmodels.ImageReference(\n",
    "            publisher=\"Canonical\",\n",
    "            offer=\"UbuntuServer\",\n",
    "            sku=\"18.04-LTS\",\n",
    "            version=\"latest\"\n",
    "        ),\n",
    "        node_agent_sku_id=\"batch.node.ubuntu 18.04\"),\n",
    "    vm_size=vmSize,\n",
    "    target_dedicated_nodes=dedicatedNodes,\n",
    "    target_low_priority_nodes=lowPriorityNodes,\n",
    "    start_task=startTask)\n",
    "\n",
    "# create pool\n",
    "try:\n",
    "    print(\"Attempting to create pool:\", pool.id)\n",
    "    batch_client.pool.add(pool)\n",
    "    print(\"Created pool:\", pool.id)\n",
    "except batchmodels.BatchErrorException as e:\n",
    "    if e.error.code != \"PoolExists\":\n",
    "        raise\n",
    "    else:\n",
    "        print(\"Pool {!r} already exists\".format(pool.id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for all nodes in pool SimulationPoolInfluenza to reach one of: [<ComputeNodeState.idle: 'idle'>, <ComputeNodeState.running: 'running'>]\n",
      "\n",
      "waiting for 5 nodes to reach desired state...\n",
      "waiting for 5 nodes to reach desired state...\n",
      "waiting for 5 nodes to reach desired state...\n",
      "waiting for 5 nodes to reach desired state...\n",
      "waiting for 5 nodes to reach desired state...\n",
      "waiting for 5 nodes to reach desired state...\n",
      "waiting for 5 nodes to reach desired state...\n",
      "waiting for 5 nodes to reach desired state...\n",
      "waiting for 5 nodes to reach desired state...\n",
      "waiting for 5 nodes to reach desired state...\n",
      "waiting for 5 nodes to reach desired state...\n",
      "tvmps_477d8a63ba8bd4433965aff979ec71f38d287e7bff4a515683e1b022e8230317_p ComputeNodeState.idle False\n",
      "tvmps_862b92057067bd391d54175ba8e5c21ad626b6129f28f6bf47ad465cdbfab8d5_p ComputeNodeState.idle False\n",
      "tvmps_b3f39852da98c4f38b637eff990f67bfe01108bdd17b21f2f969ebffe3ab4c5c_p ComputeNodeState.idle False\n",
      "tvmps_b533ace5f45d7275cdd1daa9ef76ba8faf610061572dd83e670bfa52a65ab730_p ComputeNodeState.idle False\n",
      "tvmps_d4ad895eba0f865499312af02b5a4eadb980c72d5b81a05b4115348a45ec5ace_p ComputeNodeState.idle False\n"
     ]
    }
   ],
   "source": [
    "def wait_for_all_nodes_state(batch_client, pool, node_state):\n",
    "    print('Waiting for all nodes in pool {} to reach one of: {!r}\\n'.format(\n",
    "        pool.id, node_state))\n",
    "    i = 0\n",
    "    targetNodes = pool.target_dedicated_nodes + pool.target_low_priority_nodes\n",
    "    while True:\n",
    "        # refresh pool to ensure that there is no resize error\n",
    "        pool = batch_client.pool.get(pool.id)\n",
    "        if pool.resize_errors is not None:\n",
    "            resize_errors = \"\\n\".join([repr(e) for e in pool.resize_errors])\n",
    "            raise RuntimeError(\n",
    "                'resize error encountered for pool {}:\\n{}'.format(\n",
    "                    pool.id, resize_errors))\n",
    "        nodes = list(batch_client.compute_node.list(pool.id))\n",
    "        if (len(nodes) >= targetNodes and\n",
    "                all(node.state in node_state for node in nodes)):\n",
    "            return nodes\n",
    "        i += 1\n",
    "        if i % 3 == 0:\n",
    "            print('waiting for {} nodes to reach desired state...'.format(\n",
    "                targetNodes))\n",
    "        time.sleep(10)\n",
    "\n",
    "# we check if all nodes are up before we continue\n",
    "nodes = wait_for_all_nodes_state(batch_client, pool, [batchmodels.ComputeNodeState.idle, batchmodels.ComputeNodeState.running])\n",
    "\n",
    "# show all nodes\n",
    "for n in nodes:\n",
    "    print(n.id, n.state, n.is_dedicated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Job to run on the Pool\n",
    "We will now create a job and an associated Prep task to ensure the application is downloaded, extracted to a known location and all python packages are installed via pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Preparation task:\n",
      "CommandLine: /bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && tar -xvf $AZ_BATCH_JOB_PREP_WORKING_DIR/app.tar.gz && sudo su && pip3 install -r requirements.txt \"\n",
      "ResourceFiles:\n",
      "\thttps://covidiabatchstorage.blob.core.windows.net/application/app.tar.gz?se=2020-04-15T22%3A58%3A37Z&sp=r&sv=2018-03-28&sr=b&sig=9RRpXh1rPnACsnU9YuEvuglUI%2BOBT4z/HhcO/EL5EYg%3D\n"
     ]
    }
   ],
   "source": [
    "# creating a unique job Id\n",
    "job_id = settings[\"jobIdPrefix\"] + \"_\" + datetime.now().strftime(\"%Y-%m-%d_%H-%m\")\n",
    "\n",
    "# setup the task command\n",
    "command_line = f\"/bin/bash -c \\\"cd $AZ_BATCH_NODE_SHARED_DIR && tar -xvf $AZ_BATCH_JOB_PREP_WORKING_DIR/{app_package_file_name} && sudo su && pip3 install -r requirements.txt \\\"\"\n",
    "\n",
    "# create an elevated identity to run the start task\n",
    "user = batchmodels.AutoUserSpecification(scope=batchmodels.AutoUserScope.pool, elevation_level=batchmodels.ElevationLevel.admin)\n",
    "user_identity = batchmodels.UserIdentity(auto_user=user)   \n",
    "\n",
    "# setup the start task\n",
    "jobTask = batchmodels.JobPreparationTask(\n",
    "        command_line = command_line,\n",
    "        user_identity = user_identity,\n",
    "        wait_for_success = True,\n",
    "        resource_files = [batchmodels.ResourceFile(\n",
    "                         file_path = app_package_file_name,\n",
    "                         http_url = appFileSas)])\n",
    "\n",
    "print(\"Job Preparation task:\")\n",
    "print(f\"CommandLine: {command_line}\")\n",
    "print(f\"ResourceFiles:\")\n",
    "for f in jobTask.resource_files:\n",
    "    print(f\"\\t{f.http_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating job [SimulationJobInfluenza_2020-04-14_23-04]...\n"
     ]
    }
   ],
   "source": [
    "# setup job\n",
    "job = batchmodels.JobAddParameter(\n",
    "    id=job_id,\n",
    "    pool_info=batchmodels.PoolInformation(pool_id=pool.id),\n",
    "    job_preparation_task = jobTask)\n",
    "\n",
    "# create job\n",
    "print('Creating job [{}]...'.format(job.id))\n",
    "\n",
    "try:\n",
    "    batch_client.job.add(job)\n",
    "except batchmodels.batch_error.BatchErrorException as err:\n",
    "    print_batch_exception(err)\n",
    "    if err.error.code != \"JobExists\":\n",
    "        raise\n",
    "    else:\n",
    "        print(\"Job {!r} already exists\".format(job_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Tasks to the Job\n",
    "Now that our application is correctly configured and we made sure Python is installed in all nodes, we need to setup a task to run a work item. We can launch many tasks inside the same job and Azure Batch will assign it to any VMs in the pool.\n",
    "\n",
    "In this example, we will create as many tasks as files in input_data (a local folder in this repo). This is a simple way of doing paralel processing of a large file when splits can be done. Another option is simple iterating over an array of parameter values and creating a task for each different value. We illustrate here the most complicated scenario which involves passing different input files to the script and uploading those files to the input container in the storage account.\n",
    "\n",
    "These tasks also write output to storage. The main.py script writes an output file and we configure the task to upload these files to the output container we created earlier. It is done after the task ends successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://covidiabatchstorage.blob.core.windows.net/output?se=2020-04-15T23%3A05%3A44Z&sp=w&sv=2018-03-28&sr=c&sig=xf1RT9c40Tg%2BowabSH1Y6umYZv5Fbq%2BH8vCYlEO/zfI%3D\n"
     ]
    }
   ],
   "source": [
    "# get a sas url for write access to output container. This will be used so we can persist task output files\n",
    "output_container_sas = create_container_sas_token(blob_client, container_name=output_container_name, permission=azureblob.BlobPermissions.WRITE)\n",
    "print(output_container_sas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating task Process-1\n",
      "Uploaded input_data/data56.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data56.dat -o $AZ_BATCH_TASK_WORKING_DIR/data56_output.csv\"\n",
      "\n",
      "Creating task Process-2\n",
      "Uploaded input_data/data43.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data43.dat -o $AZ_BATCH_TASK_WORKING_DIR/data43_output.csv\"\n",
      "\n",
      "Creating task Process-3\n",
      "Uploaded input_data/data8.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data8.dat -o $AZ_BATCH_TASK_WORKING_DIR/data8_output.csv\"\n",
      "\n",
      "Creating task Process-4\n",
      "Uploaded input_data/data35.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data35.dat -o $AZ_BATCH_TASK_WORKING_DIR/data35_output.csv\"\n",
      "\n",
      "Creating task Process-5\n",
      "Uploaded input_data/data33.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data33.dat -o $AZ_BATCH_TASK_WORKING_DIR/data33_output.csv\"\n",
      "\n",
      "Creating task Process-6\n",
      "Uploaded input_data/data23.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data23.dat -o $AZ_BATCH_TASK_WORKING_DIR/data23_output.csv\"\n",
      "\n",
      "Creating task Process-7\n",
      "Uploaded input_data/data25.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data25.dat -o $AZ_BATCH_TASK_WORKING_DIR/data25_output.csv\"\n",
      "\n",
      "Creating task Process-8\n",
      "Uploaded input_data/data34.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data34.dat -o $AZ_BATCH_TASK_WORKING_DIR/data34_output.csv\"\n",
      "\n",
      "Creating task Process-9\n",
      "Uploaded input_data/data57.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data57.dat -o $AZ_BATCH_TASK_WORKING_DIR/data57_output.csv\"\n",
      "\n",
      "Creating task Process-10\n",
      "Uploaded input_data/data6.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data6.dat -o $AZ_BATCH_TASK_WORKING_DIR/data6_output.csv\"\n",
      "\n",
      "Creating task Process-11\n",
      "Uploaded input_data/data59.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data59.dat -o $AZ_BATCH_TASK_WORKING_DIR/data59_output.csv\"\n",
      "\n",
      "Creating task Process-12\n",
      "Uploaded input_data/data3.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data3.dat -o $AZ_BATCH_TASK_WORKING_DIR/data3_output.csv\"\n",
      "\n",
      "Creating task Process-13\n",
      "Uploaded input_data/data30.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data30.dat -o $AZ_BATCH_TASK_WORKING_DIR/data30_output.csv\"\n",
      "\n",
      "Creating task Process-14\n",
      "Uploaded input_data/data47.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data47.dat -o $AZ_BATCH_TASK_WORKING_DIR/data47_output.csv\"\n",
      "\n",
      "Creating task Process-15\n",
      "Uploaded input_data/data10.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data10.dat -o $AZ_BATCH_TASK_WORKING_DIR/data10_output.csv\"\n",
      "\n",
      "Creating task Process-16\n",
      "Uploaded input_data/data26.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data26.dat -o $AZ_BATCH_TASK_WORKING_DIR/data26_output.csv\"\n",
      "\n",
      "Creating task Process-17\n",
      "Uploaded input_data/data48.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data48.dat -o $AZ_BATCH_TASK_WORKING_DIR/data48_output.csv\"\n",
      "\n",
      "Creating task Process-18\n",
      "Uploaded input_data/data31.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data31.dat -o $AZ_BATCH_TASK_WORKING_DIR/data31_output.csv\"\n",
      "\n",
      "Creating task Process-19\n",
      "Uploaded input_data/data28.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data28.dat -o $AZ_BATCH_TASK_WORKING_DIR/data28_output.csv\"\n",
      "\n",
      "Creating task Process-20\n",
      "Uploaded input_data/data2.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data2.dat -o $AZ_BATCH_TASK_WORKING_DIR/data2_output.csv\"\n",
      "\n",
      "Creating task Process-21\n",
      "Uploaded input_data/data58.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data58.dat -o $AZ_BATCH_TASK_WORKING_DIR/data58_output.csv\"\n",
      "\n",
      "Creating task Process-22\n",
      "Uploaded input_data/data13.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data13.dat -o $AZ_BATCH_TASK_WORKING_DIR/data13_output.csv\"\n",
      "\n",
      "Creating task Process-23\n",
      "Uploaded input_data/data46.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data46.dat -o $AZ_BATCH_TASK_WORKING_DIR/data46_output.csv\"\n",
      "\n",
      "Creating task Process-24\n",
      "Uploaded input_data/data29.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data29.dat -o $AZ_BATCH_TASK_WORKING_DIR/data29_output.csv\"\n",
      "\n",
      "Creating task Process-25\n",
      "Uploaded input_data/data41.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data41.dat -o $AZ_BATCH_TASK_WORKING_DIR/data41_output.csv\"\n",
      "\n",
      "Creating task Process-26\n",
      "Uploaded input_data/data44.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data44.dat -o $AZ_BATCH_TASK_WORKING_DIR/data44_output.csv\"\n",
      "\n",
      "Creating task Process-27\n",
      "Uploaded input_data/data39.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data39.dat -o $AZ_BATCH_TASK_WORKING_DIR/data39_output.csv\"\n",
      "\n",
      "Creating task Process-28\n",
      "Uploaded input_data/data49.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data49.dat -o $AZ_BATCH_TASK_WORKING_DIR/data49_output.csv\"\n",
      "\n",
      "Creating task Process-29\n",
      "Uploaded input_data/data20.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data20.dat -o $AZ_BATCH_TASK_WORKING_DIR/data20_output.csv\"\n",
      "\n",
      "Creating task Process-30\n",
      "Uploaded input_data/data19.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data19.dat -o $AZ_BATCH_TASK_WORKING_DIR/data19_output.csv\"\n",
      "\n",
      "Creating task Process-31\n",
      "Uploaded input_data/data38.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data38.dat -o $AZ_BATCH_TASK_WORKING_DIR/data38_output.csv\"\n",
      "\n",
      "Creating task Process-32\n",
      "Uploaded input_data/data15.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data15.dat -o $AZ_BATCH_TASK_WORKING_DIR/data15_output.csv\"\n",
      "\n",
      "Creating task Process-33\n",
      "Uploaded input_data/data37.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data37.dat -o $AZ_BATCH_TASK_WORKING_DIR/data37_output.csv\"\n",
      "\n",
      "Creating task Process-34\n",
      "Uploaded input_data/data24.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data24.dat -o $AZ_BATCH_TASK_WORKING_DIR/data24_output.csv\"\n",
      "\n",
      "Creating task Process-35\n",
      "Uploaded input_data/data12.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data12.dat -o $AZ_BATCH_TASK_WORKING_DIR/data12_output.csv\"\n",
      "\n",
      "Creating task Process-36\n",
      "Uploaded input_data/data7.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data7.dat -o $AZ_BATCH_TASK_WORKING_DIR/data7_output.csv\"\n",
      "\n",
      "Creating task Process-37\n",
      "Uploaded input_data/data53.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data53.dat -o $AZ_BATCH_TASK_WORKING_DIR/data53_output.csv\"\n",
      "\n",
      "Creating task Process-38\n",
      "Uploaded input_data/data11.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data11.dat -o $AZ_BATCH_TASK_WORKING_DIR/data11_output.csv\"\n",
      "\n",
      "Creating task Process-39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded input_data/data40.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data40.dat -o $AZ_BATCH_TASK_WORKING_DIR/data40_output.csv\"\n",
      "\n",
      "Creating task Process-40\n",
      "Uploaded input_data/data18.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data18.dat -o $AZ_BATCH_TASK_WORKING_DIR/data18_output.csv\"\n",
      "\n",
      "Creating task Process-41\n",
      "Uploaded input_data/data52.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data52.dat -o $AZ_BATCH_TASK_WORKING_DIR/data52_output.csv\"\n",
      "\n",
      "Creating task Process-42\n",
      "Uploaded input_data/data9.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data9.dat -o $AZ_BATCH_TASK_WORKING_DIR/data9_output.csv\"\n",
      "\n",
      "Creating task Process-43\n",
      "Uploaded input_data/data54.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data54.dat -o $AZ_BATCH_TASK_WORKING_DIR/data54_output.csv\"\n",
      "\n",
      "Creating task Process-44\n",
      "Uploaded input_data/data16.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data16.dat -o $AZ_BATCH_TASK_WORKING_DIR/data16_output.csv\"\n",
      "\n",
      "Creating task Process-45\n",
      "Uploaded input_data/data45.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data45.dat -o $AZ_BATCH_TASK_WORKING_DIR/data45_output.csv\"\n",
      "\n",
      "Creating task Process-46\n",
      "Uploaded input_data/data55.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data55.dat -o $AZ_BATCH_TASK_WORKING_DIR/data55_output.csv\"\n",
      "\n",
      "Creating task Process-47\n",
      "Uploaded input_data/data21.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data21.dat -o $AZ_BATCH_TASK_WORKING_DIR/data21_output.csv\"\n",
      "\n",
      "Creating task Process-48\n",
      "Uploaded input_data/data36.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data36.dat -o $AZ_BATCH_TASK_WORKING_DIR/data36_output.csv\"\n",
      "\n",
      "Creating task Process-49\n",
      "Uploaded input_data/data51.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data51.dat -o $AZ_BATCH_TASK_WORKING_DIR/data51_output.csv\"\n",
      "\n",
      "Creating task Process-50\n",
      "Uploaded input_data/data32.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data32.dat -o $AZ_BATCH_TASK_WORKING_DIR/data32_output.csv\"\n",
      "\n",
      "Creating task Process-51\n",
      "Uploaded input_data/data4.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data4.dat -o $AZ_BATCH_TASK_WORKING_DIR/data4_output.csv\"\n",
      "\n",
      "Creating task Process-52\n",
      "Uploaded input_data/data50.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data50.dat -o $AZ_BATCH_TASK_WORKING_DIR/data50_output.csv\"\n",
      "\n",
      "Creating task Process-53\n",
      "Uploaded input_data/data1.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data1.dat -o $AZ_BATCH_TASK_WORKING_DIR/data1_output.csv\"\n",
      "\n",
      "Creating task Process-54\n",
      "Uploaded input_data/data14.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data14.dat -o $AZ_BATCH_TASK_WORKING_DIR/data14_output.csv\"\n",
      "\n",
      "Creating task Process-55\n",
      "Uploaded input_data/data22.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data22.dat -o $AZ_BATCH_TASK_WORKING_DIR/data22_output.csv\"\n",
      "\n",
      "Creating task Process-56\n",
      "Uploaded input_data/data5.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data5.dat -o $AZ_BATCH_TASK_WORKING_DIR/data5_output.csv\"\n",
      "\n",
      "Creating task Process-57\n",
      "Uploaded input_data/data27.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data27.dat -o $AZ_BATCH_TASK_WORKING_DIR/data27_output.csv\"\n",
      "\n",
      "Creating task Process-58\n",
      "Uploaded input_data/data0.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data0.dat -o $AZ_BATCH_TASK_WORKING_DIR/data0_output.csv\"\n",
      "\n",
      "Creating task Process-59\n",
      "Uploaded input_data/data42.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data42.dat -o $AZ_BATCH_TASK_WORKING_DIR/data42_output.csv\"\n",
      "\n",
      "Creating task Process-60\n",
      "Uploaded input_data/data17.dat to container input\n",
      "/bin/bash -c \"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/data17.dat -o $AZ_BATCH_TASK_WORKING_DIR/data17_output.csv\"\n"
     ]
    }
   ],
   "source": [
    "# we get a list of input files\n",
    "file_list = glob.glob(\"input_data/*.dat\")\n",
    "\n",
    "# initialize task counter\n",
    "i = 0\n",
    "for f in file_list:\n",
    "    # increment task counter\n",
    "    i = i + 1\n",
    "    \n",
    "    # create a task id\n",
    "    task_id = \"Process-\" + str(i)\n",
    "    print(\"\\nCreating task\",task_id)\n",
    "    \n",
    "    # grab file name\n",
    "    input_file = f.split(\"/\")[-1:][0]\n",
    "    output_file = input_file.replace(\".dat\",\"_output.csv\")\n",
    "    \n",
    "    # upload file to azure storage\n",
    "    input_file_sas = upload_blob_and_create_sas(blob_client, input_container_name, f, input_file)\n",
    "    \n",
    "    # setup task command\n",
    "    taskCommand = f\"/bin/bash -c \\\"cd $AZ_BATCH_NODE_SHARED_DIR && python3 main.py -i $AZ_BATCH_TASK_WORKING_DIR/{input_file} -o $AZ_BATCH_TASK_WORKING_DIR/{output_file}\\\"\"\n",
    "    print(taskCommand)\n",
    "    \n",
    "    # create an elevated identity to run the start task\n",
    "    user = batchmodels.AutoUserSpecification(scope=batchmodels.AutoUserScope.pool, elevation_level=batchmodels.ElevationLevel.admin)\n",
    "    user_identity = batchmodels.UserIdentity(auto_user=user)   \n",
    "   \n",
    "    # setup output files destination\n",
    "    containerDest = batchmodels.OutputFileBlobContainerDestination(container_url = output_container_sas, path = task_id)\n",
    "    outputFileDestination = batchmodels.OutputFileDestination(container = containerDest)\n",
    "    \n",
    "    # setup output files upload condition\n",
    "    uploadCondition = batchmodels.OutputFileUploadCondition.task_success\n",
    "    uploadOptions = batchmodels.OutputFileUploadOptions(upload_condition = uploadCondition)\n",
    "    \n",
    "    # output files\n",
    "    output_files = [batchmodels.OutputFile(destination = outputFileDestination,\n",
    "                                        upload_options = uploadOptions,\n",
    "                                        file_pattern=\"*output.csv\")]\n",
    "    \n",
    "    \n",
    "    # create task\n",
    "    task = batchmodels.TaskAddParameter(\n",
    "    id = task_id,\n",
    "    command_line=taskCommand,\n",
    "    user_identity=user_identity,\n",
    "    resource_files=[batchmodels.ResourceFile(\n",
    "                        file_path=input_file,\n",
    "                        http_url=input_file_sas)],\n",
    "    output_files=output_files)\n",
    "    \n",
    "    \n",
    "    batch_client.task.add(job_id=job.id, task=task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if all tasks are complete...\n",
      "Checking if all tasks are complete...\n",
      "Checking if all tasks are complete...\n",
      "Checking if all tasks are complete...\n",
      "Checking if all tasks are complete...\n",
      "Checking if all tasks are complete...\n",
      "All Tasks Complete!\n"
     ]
    }
   ],
   "source": [
    "def wait_for_tasks_to_complete(batch_client, job_id, timeout):\n",
    "\n",
    "    time_to_timeout_at = datetime.now() + timeout\n",
    "\n",
    "    while datetime.now() < time_to_timeout_at:\n",
    "        print(\"Checking if all tasks are complete...\")\n",
    "        tasks = batch_client.task.list(job_id)\n",
    "\n",
    "        incomplete_tasks = [task for task in tasks if\n",
    "                            task.state != batchmodels.TaskState.completed]\n",
    "        if not incomplete_tasks:\n",
    "            return\n",
    "        time.sleep(30)\n",
    "\n",
    "    raise TimeoutError(\"Timed out waiting for tasks to complete\")\n",
    "\n",
    "wait_for_tasks_to_complete(batch_client, job.id, timedelta(minutes=60))\n",
    "print(\"All Tasks Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions to read task output directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_stream_as_string(stream, encoding):\n",
    "    output = io.BytesIO()\n",
    "    try:\n",
    "        for data in stream:\n",
    "            output.write(data)\n",
    "        if encoding is None:\n",
    "            encoding = 'utf-8'\n",
    "        return output.getvalue().decode(encoding)\n",
    "    finally:\n",
    "        output.close()\n",
    "    raise RuntimeError('could not write data to stream or decode bytes')\n",
    "\n",
    "def read_task_file_as_string(batch_client, job_id, task_id, file_name, encoding=None):\n",
    "    stream = batch_client.file.get_from_task(job_id, task_id, file_name)\n",
    "    return read_stream_as_string(stream, encoding)\n",
    "\n",
    "def print_task_output(batch_client, job_id, task_ids, encoding=None):\n",
    "    _STANDARD_OUT_FILE_NAME = 'stdout.txt'\n",
    "    _STANDARD_ERROR_FILE_NAME = 'stderr.txt'\n",
    "    \n",
    "    for task_id in task_ids:\n",
    "        file_text = read_task_file_as_string(\n",
    "            batch_client,\n",
    "            job_id,\n",
    "            task_id,\n",
    "            _STANDARD_OUT_FILE_NAME,\n",
    "            encoding)\n",
    "        print(\"{} content for task {}: \".format(\n",
    "            _STANDARD_OUT_FILE_NAME,\n",
    "            task_id))\n",
    "        print(file_text)\n",
    "\n",
    "        file_text = read_task_file_as_string(\n",
    "            batch_client,\n",
    "            job_id,\n",
    "            task_id,\n",
    "            _STANDARD_ERROR_FILE_NAME,\n",
    "            encoding)\n",
    "        print(\"{} content for task {}: \".format(\n",
    "            _STANDARD_ERROR_FILE_NAME,\n",
    "            task_id))\n",
    "        print(file_text)\n",
    "\n",
    "tasks = batch_client.task.list(job_id)\n",
    "task_ids = [task.id for task in tasks]\n",
    "\n",
    "# let's print the output of the first 3 tasks\n",
    "print_task_output(batch_client, job_id, task_ids[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Output\n",
    "As we created each task with an output file option, the produced filed by each execution of our sample_application will result in a new file being created in the output container in Azure Storage. We can quickly check all the files here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_list = blob_client.list_blobs(container_name=output_container_name)\n",
    "print(\"Number of files:\",len(list(output_file_list)))\n",
    "print(\"\\nFirst 10:\")\n",
    "for f in list(output_file_list)[0:10]:\n",
    "    print(f.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Job\n",
    "No issues in removing the job because each task will write it's results to the output container in Azure Storage, however, keeping this uncommented will allow you to see the job in Batch Explorer and debug any failed tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_client.job.delete(job.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Pool\n",
    "Note: you may not necessarily want to do this because creating the pool takes some time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_client.pool.delete(pool.id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureml_py36_automl",
   "language": "python",
   "name": "conda-env-azureml_py36_automl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
